import os
import streamlit as st #  streamlit = Pythonì—ì„œ GUI ìƒì„±
import openai
import googletrans
import playsound
import pyaudio
import wave

from side_bar import run_side_tap_draw
from dotenv import load_dotenv # OPEN_API_KEY

# -------

# .env íŒŒì¼ë¡œë¶€í„° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ì‚¬ì´ë“œ ë°” ìƒì„±
run_side_tap_draw()
sample_rate = 44100  # ì˜¤ë””ì˜¤ ìƒ˜í”Œ ì†ë„
duration = 6  # ë…¹ìŒ ì‹œê°„ (ì´ˆ)

# ìŠ¤íŠ¸ë¦¼ë¦¿ ì•± í—¤ë” ì„¤ì •
st.header("ê·¸ë¦¼ ê·¸ë¦¬ê¸° ğŸ¨ AI Toryê°€ ê·¸ë¦¼ì„ ê·¸ë ¤ì¤„ê²Œìš” ğŸª„")
st.caption('í† ë¦¬ê°€ í•™ìŠµí•œ ì •ë³´ë¡œ ê·¸ë¦¼ì„ ê·¸ë ¤ì¤„ê²Œìš” ğŸ”¥')

st.markdown("""
<style>
    .stButton > button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

if not st.session_state.get('text') or st.session_state.text == []:
    st.warning("í† ë¦¬ì™€ ëŒ€í™”ë¥¼ ë¨¼ì € í•´ì£¼ì„¸ìš”.")

# st.write(st.session_state.get('text'))

if st.session_state.get('text'):
    query = st.text_input("ì›í•˜ëŠ” ê·¸ë¦¼ì— ëŒ€í•´ì„œ ì„¤ëª… í•´ì£¼ì„¸ìš”!", placeholder="Send a message")
    whisper_button = st.button("ğŸ™ï¸", help="ë§ˆì´í¬ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”.", key="unique_key_for_whisper_button")

    if whisper_button:
                    with st.spinner("ë§í•´ì£¼ì„¸ìš”! í† ë¦¬ê°€ ë“£ê³ ìˆì–´ìš”..."):
                        # PyAudioë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
                        audio_data = []
                        p = pyaudio.PyAudio()
                        stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)

                        # ì˜¤ë””ì˜¤ ë°ì´í„° ë…¹ìŒ
                        for i in range(0, int(sample_rate / 1024 * duration)):
                            audio_chunk = stream.read(1024)
                            audio_data.append(audio_chunk)

                    with st.spinner("í† ë¦¬ê°€ ë‹¤ ë“¤ì—ˆì–´ìš”..."):
                        # ë…¹ìŒ ì¤‘ì§€
                        stream.stop_stream()
                        stream.close()
                        p.terminate()

                        # ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
                        audio_file = "recorded_audio.wav"
                        with wave.open(audio_file, "wb") as wf:
                            wf.setnchannels(1)
                            wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))
                            wf.setframerate(sample_rate)
                            wf.writeframes(b"".join(audio_data))

                        # ìˆ˜ì •ëœ ë¶€ë¶„: ë…¹ìŒëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ê¸° ëª¨ë“œë¡œ ì—´ê¸°
                        with open("recorded_audio.wav", "rb") as audio_file:
                            transcript = openai.Audio.transcribe("whisper-1", audio_file)
                            ko_response = transcript["text"].encode('utf-16').decode('utf-16')
                            query = ko_response
    if query:
            conversation = ""
            conversation += f"{st.session_state['text']}"
            user_input = conversation

            gpt_prompt = [{
                "role" : "system", 
                "content" : f"""
                            [Information] = {user_input}.
                            You are Beatrix Potter.
                            You are draw a children's comic draw pic about {query}.
                            Imagine the details, No need text.
                            You Must be drawing for children's.
                            If say something that's not in the [Information], just draw your own picture of {query}
                            """
            }]
            
            # gpt_prompt.append({
            #         "role" : "user",
            #         "content" : query
            # })

            with st.spinner("í† ë¦¬ê°€ ë™í™”ë¥¼ ìƒìƒí•˜ê³  ìˆì–´ìš”.."):
                    gpt_response = openai.ChatCompletion.create(
                        model="gpt-3.5-turbo-16k",
                        messages=gpt_prompt,
                        max_tokens=150
                    )

            pic_prompt = gpt_response["choices"][0]["message"]["content"]
            dalle_prompt = pic_prompt

            with st.spinner("í† ë¦¬ê°€ ë™í™”ì— ëŒ€í•´ì„œ ê·¸ë ¤ì¤„ê²Œìš”.."):
                dallE_response = openai.Image.create(
                    prompt=dalle_prompt,
                    size= "512x512",
                    n=4
                )
                
            translator = googletrans.Translator()
            ko_dalle_prompt = translator.translate(dalle_prompt, dest='ko')

            # ì´ë¯¸ì§€ë¥¼ 2x2 ë°°ì—´ë¡œ ì •ë ¬
            num_images = len(dallE_response["data"])
            if num_images >= 4:
                # 2x2 ë°°ì—´ì„ ë§Œë“¤ê¸° ìœ„í•´ 4ê°œì˜ ì´ë¯¸ì§€ë¥¼ 2ê°œì”© ë‚˜ëˆ”
                rows = [dallE_response["data"][:2], dallE_response["data"][2:4]]
            else:
                rows = [dallE_response["data"]]  # 4ê°œ ë¯¸ë§Œì˜ ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ í‘œì‹œ

            # ê° í–‰ì— ì´ë¯¸ì§€ ë°°ì¹˜
            for row_images in rows:
                columns = st.columns(len(row_images))
                for i, image_data in enumerate(row_images):
                    with columns[i]:
                        st.image(image_data["url"])

            st.success(ko_dalle_prompt.text)

