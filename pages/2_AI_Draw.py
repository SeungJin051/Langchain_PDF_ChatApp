import os
import streamlit as st #  streamlit = Pythonì—ì„œ GUI ìƒì„±
import openai
import googletrans
import playsound
import pyaudio
import wave

from side_bar import run_side_tap_draw, set_bg_hack
from dotenv import load_dotenv # OPEN_API_KEY

# -------

# .env íŒŒì¼ë¡œë¶€í„° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
main_bg_ext = "pages/images/tory_back.png"

set_bg_hack(main_bg_ext)
# ì‚¬ì´ë“œ ë°” ìƒì„±
run_side_tap_draw()
sample_rate = 44100  # ì˜¤ë””ì˜¤ ìƒ˜í”Œ ì†ë„
duration = 6  # ë…¹ìŒ ì‹œê°„ (ì´ˆ)

# ìŠ¤íŠ¸ë¦¼ë¦¿ ì•± í—¤ë” ì„¤ì •
st.header("ğŸ¨ AIê°€ ê·¸ë¦¼ì„ ê·¸ë ¤ì¤„ê²Œìš” ğŸª„")
st.caption('AIê°€ ê·¸ë¦¼ì„ ê·¸ë ¤ì¤„ê²Œìš”.. ğŸ”¥')

st.write( """
        <style>
        .st-bd {
            background-color: rgba(255, 255, 255);
            border-radius: 15px; 
            padding: 30px; 
            box-shadow: 5px 5px 5px rgba(0, 0, 0, 0.2); 
        }
        </style>
        """,
    unsafe_allow_html=True)

tab1, tab2 = st.tabs([" ", " "])
# if not st.session_state.get('text') or st.session_state.text == []:
#     st.warning("í† ë¦¬ì™€ ëŒ€í™”ë¥¼ ë¨¼ì € í•´ì£¼ì„¸ìš”.")

# st.write(st.session_state.get('text'))

# if st.session_state.get('text'):

with tab1:
    with st.form(key='query_form', clear_on_submit=True):
        query = st.text_input("ì›í•˜ëŠ” ê·¸ë¦¼ì— ëŒ€í•´ì„œ ë¬˜ì‚¬ í•´ì£¼ì„¸ìš”!", placeholder="Send a message", value="")
        col1, col2, col3, col4, col5, col6 = st.columns([1, 1, 1, 1, 1, 1])
        with col1:
            submit = st.form_submit_button(label="Send")

        with col2:
            whisper_button = st.form_submit_button("ğŸ™ï¸", help="ë§ˆì´í¬ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”.")
        


    if whisper_button:
                    with st.spinner("ë§í•´ì£¼ì„¸ìš”! AIê°€ ë“£ê³ ìˆì–´ìš”..."):
                        # PyAudioë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
                        audio_data = []
                        p = pyaudio.PyAudio()
                        stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)

                        # ì˜¤ë””ì˜¤ ë°ì´í„° ë…¹ìŒ
                        for i in range(0, int(sample_rate / 1024 * duration)):
                            audio_chunk = stream.read(1024)
                            audio_data.append(audio_chunk)

                    with st.spinner("AIê°€ ë‹¤ ë“¤ì—ˆì–´ìš”..."):
                        # ë…¹ìŒ ì¤‘ì§€
                        stream.stop_stream()
                        stream.close()
                        p.terminate()

                        # ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
                        audio_file = "recorded_audio.wav"
                        with wave.open(audio_file, "wb") as wf:
                            wf.setnchannels(1)
                            wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))
                            wf.setframerate(sample_rate)
                            wf.writeframes(b"".join(audio_data))

                        # ìˆ˜ì •ëœ ë¶€ë¶„: ë…¹ìŒëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ê¸° ëª¨ë“œë¡œ ì—´ê¸°
                        with open("recorded_audio.wav", "rb") as audio_file:
                            transcript = openai.Audio.transcribe("whisper-1", audio_file)
                            ko_response = transcript["text"].encode('utf-16').decode('utf-16')
                            query = ko_response
    if query:
            # conversation = ""
            # conversation += f"{st.session_state['text']}"
            # user_input = conversation
            # PDFê°€ ì—…ë¡œë“œë˜ì—ˆë‹¤ë©´ PDF ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤
            gpt_prompt = [{
                    "role" : "system", 
                    "content" : "understand Korean language in English. Imagine the detail appareance of the input. Response it shortly around 50 words." # ëª…ë ¹
            }]
            gpt_prompt.append({
                    "role" : "user",
                    "content" :f"{query}"
            })

            with st.spinner("AIê°€ ì‚¬ìš©ìì˜ ê·¸ë¦¼ì„ ìƒìƒí•˜ê³  ìˆì–´ìš”.."):
                    gpt_response = openai.ChatCompletion.create(
                        model="gpt-3.5-turbo-16k",
                        messages=gpt_prompt,
                        max_tokens=150
                    )

            pic_prompt = gpt_response["choices"][0]["message"]["content"]
            dalle_prompt = pic_prompt

            with st.spinner("AIê°€ ì‚¬ìš©ìì˜ ê·¸ë¦¼ì— ëŒ€í•´ì„œ ê·¸ë ¤ì¤„ê²Œìš”.."):
                dallE_response = openai.Image.create(
                    prompt=dalle_prompt,
                    size= "512x512",
                    n=4
                )
                
            translator = googletrans.Translator()
            ko_dalle_prompt = translator.translate(dalle_prompt, dest='ko')

            # ì´ë¯¸ì§€ë¥¼ 2x2 ë°°ì—´ë¡œ ì •ë ¬
            num_images = len(dallE_response["data"])
            if num_images >= 4:
                # 2x2 ë°°ì—´ì„ ë§Œë“¤ê¸° ìœ„í•´ 4ê°œì˜ ì´ë¯¸ì§€ë¥¼ 2ê°œì”© ë‚˜ëˆ”
                rows = [dallE_response["data"][:2], dallE_response["data"][2:4]]
            else:
                rows = [dallE_response["data"]]  # 4ê°œ ë¯¸ë§Œì˜ ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ í‘œì‹œ

            # ê° í–‰ì— ì´ë¯¸ì§€ ë°°ì¹˜
            for row_images in rows:
                columns = st.columns(len(row_images))
                for i, image_data in enumerate(row_images):
                    with columns[i]:
                        st.image(image_data["url"])

            st.success(ko_dalle_prompt.text)

